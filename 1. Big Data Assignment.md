# Big Data Assignment
## _Samantha Knox C 1000466_

## Contents:
| Part 1 
| Part 2 
| Part 3 
| Appendix A 
| Appendix B |

## - ✨Part 1✨ 

##### Login: root  /  Password: Sammy100!

### Notes and relections on Part 1:

- I had to re-do all of my scripts when I got to part 2 as I realised that I had used  " , "  as a delimiter. This caused me problems, as the tweet text strings contained many commas. I replaced my delimiter with  " | "  which recified the issue. 
-  In order to repeat the exercise on all 25 files, I used an asterix in place of the file extension, which searched all file extensions starting with the term I was interested in, i.e. //TWEET*.csv.
-  I could not find a way within pig to include the column headings in my TWEET_RETWEET and TWEET_MENTIONS outputs. This caused me problems in Part 2 where I had to manually define the column headers 
- Scripts are saved in HDFS (maria_dev/maria_dev) in this location: (apologies for the other files, I was unable to delete them) 
> root/root/user/Uni_Assignment 
- See Appendix A for a copy of Pig Scripts (TWEET, RETWEET, MENTIONS, HASHTAGS_TIMELINES and TWEET_RETWEET)
- A back-up hard copy of the output .csv files I created (TWEET_RETWEET & TWEET_MENTIONS) for part 2 can also be found here:
https://github.com/Knoxyherself/Big-Data-Assignment

## - ✨Part  2✨

### When I first used Spark, it automatically logged me in using my Google account. I am not sure if you will be able to access my notebook. 

### Notes and relections on Part 2:

Before starting Part 2, I manually saved my TWEET_RETWEET and TWEET_MENTIONS output files into my HDFS 'maria_dev' folder here: 
> /user/maria_dev/TWEET_RETWEET 

as I had permissions issues with my root folder. Once in HDFS, I was able to read the data from Spark. Although I used the .csv format, my file did not contain any headers, so I had to add these manually using their position index. I also filtered out the columns that I wouldn't need for Part 2. 

```
%spark2.pyspark
sc

data = spark.read.format("csv").option("header", True).option("multiLine", True).option("ignoreTrailingWhiteSpace", True).load("hdfs://sandbox-hdp.hortonworks.com:8020/user/maria_dev/TWEET_RETWEET")

from pyspark.sql import Row

csvDF = rdd.map(lambda x: Row(Index = str(x.split("|")[0]),
Context = str(x.split("|")[1]),
Date = str(x.split("|")[2]),
Tweet_ID = str(x.split("|")[3]),
Retweet = str(x.split("|")[5]),
Username = str(x.split("|")[10]))).toDF()

csvDF.show()
```

This code worked and produced a table in Spark with the columns that I needed for part 2. However when I tried to run it again, it would not work and I got an error. The spark code within the "Spark Intro's" and "Data Worker" folder also stopped working, so I'm not sure if I had a problem with the Spark program overall. I tried for several days to move past this point, but could not.  

If I had been able to run the code and continue, I would followed this process:

-The file was loaded as an RDD, so it can be distributed to a number of workers. I would parallelize and apply the lambda function using map to the dataset (to allow spark to work on any chunk of data within the list) and filter for each separate context ("jeans-for-genes-day-2018" etc.) For each context I would then apply a count() function to the number of tweets weight (and mentions) and limit my result to the ten highest (take(10)). This would show me the 10 most interesting users for each context. 

My Python Spark funtion can be found here:
http://localhost:9995/#/notebook/2G1CQP8R6

## - ✨Part  3✨

### Notes and relections on Part 3:

I used Paolos uu.csv file for Part 3 and uploaded the .csv file to github, then I loaded it from here to Spark. as I could not import the csv data for neo4j onto my local machine using the "import" folder. 

I was able to produce my graph database which represented the network using:

```
load csv with headers from 'https://raw.githubusercontent.com/Knoxyherself/Big-Data-Assignment/main/uu.csv' AS row
CREATE (a:Tweeter { Name:row.from})
CREATE (b:Tweeter { Name:row.to})
CREATE (a)-[r:TWEET {Weight:row.weight}]->(b)
```

After this point, I could not get my code to work, however this is the approach that I would take:

Create a Cypher graph to group my tweets for each of the context networks:
```
CALL gds.graph.create.cypher(
    'jeans-for-genes-2018',

MATCH (n) RETURN id(n) AS id
MATCH (n)-->(m) RETURN id(n) AS source, id(m) AS target
 ```
I would have to do this 25 times for each of the different contexts.  
I would then carry out Label Propagation Community Detection:

```
CALL gds.graph.create(
    'myGraph',
    'User',
    'FOLLOW',
    {
        nodeProperties: 'seed_label',
        relationshipProperties: 'weight'
    }
)

```

This would give each node a unique community identifier label (context) which would then be propagated throughout the network. At every iteration of the propagation, every node updates its label to match the one that most of its neighbours contains. The label propagation algorithm converges when each node has the same label as most of its neighbours. Communities develop as dense groups of connected nodes aquire the same label.


I would then carry out Degree Centrality for each node, to show me the number of incoming and outgoing relationships from a node.

```
CALL gds.alpha.degree.write(configuration: Map)
YIELD nodes, createMillis, computeMillis, writeMillis, writeProperty, centralityDistribution
```
This would tell me which are the most popular nodes in the graph. 

## - ✨Appendix A - Pig Scripts✨

TWEET

```Define CSVLoader org.apache.pig.piggybank.storage.CSVLoader();                                                                 
TWEET = LOAD 'hdfs://sandbox-hdp.hortonworks.com:8020/user/coursework/data/data/TWEET*.csv' USING CSVLoader()                  
AS (index:int,context:chararray,date:chararray,tw_id:chararray,is_media:chararray,is_retweet:chararray,no_likes:chararray,no_re
tweets:chararray,reply:chararray,text:chararray,user_name:chararray);                                                          
DESCRIBE TWEET;                                                                                                                
STORE TWEET INTO 'root/user/Uni_Assignment/TWEET' USING PigStorage('|'); 
```

RETWEET

```Define CSVLoader org.apache.pig.piggybank.storage.CSVLoader();                                                                 
RETWEET = LOAD 'hdfs://sandbox-hdp.hortonworks.com:8020/user/coursework/data/data/RETWEET*.csv' USING CSVLoader()              
AS (index:int,rt_id:chararray,context:chararray,rt_user:chararray,tw_id:chararray);                                            
DESCRIBE RETWEET;                                                                                                              
STORE RETWEET INTO 'root/user/Uni_Assignment/RETWEET' USING PigStorage('|'); 
```

MENTIONS

```Define CSVLoader org.apache.pig.piggybank.storage.CSVLoader();                                                                 
MENTIONS = LOAD 'hdfs://sandbox-hdp.hortonworks.com:8020/user/coursework/data/data/MENTIONS*.csv' USING CSVLoader()            
AS (index:int,context:chararray,mentions:chararray,tw_id:chararray);                                                           
DESCRIBE MENTIONS;                                                                                                             
STORE MENTIONS INTO 'root/user/Uni_Assignment/MENTIONS' USING PigStorage('|'); 
```

HASHTAGS_TIMELINES

```Define CSVLoader org.apache.pig.piggybank.storage.CSVLoader();                                                                 
HASHTAGS_TIMELINES = LOAD 'hdfs://sandbox-hdp.hortonworks.com:8020/user/coursework/data/data/HASHTAGS_TIMELINES.csv'           
USING CSVLoader()                                                                                                              
AS (index:int,hashtag:chararray,tw_id:chararray,user_id:chararray);                                                            
DESCRIBE HASHTAGS_TIMELINES;                                                                                                   
STORE HASHTAGS_TIMELINES INTO 'root/user/Uni_Assignment/HASHTAGS_TIMELINES' USING PigStorage('|');  
```

TWEET_RETWEET

```Define CSVLoader org.apache.pig.piggybank.storage.CSVLoader();                                                                 
tweetage = LOAD 'hdfs://sandbox-hdp.hortonworks.com:8020/user/coursework/data/data/TWEET*.csv' USING CSVLoader();              
tweetraw = FILTER tweetage BY $0>1;                                                                                            
tweetdetails = FOREACH tweetraw GENERATE $0 AS index, $1 AS context, $2 AS date, $3 AS tw_id, $4 AS is_media, $5 AS is_retweet,
 $6 AS no_likes, $7 AS no_retweets, $8 AS reply, $9 AS text, $10 AS user_name;                                                 
                                                                                                                               
retweetage = LOAD 'hdfs://sandbox-hdp.hortonworks.com:8020/user/coursework/data/data/RETWEET*.csv' USING CSVLoader();          
retweetraw = FILTER retweetage BY $0>1;                                                                                        
retweetdetails = FOREACH retweetraw GENERATE $0 AS index, $1 AS rt_id, $2 AS context, $3 AS rt_user, $4 AS tw_id;              
                                                                                                                               
jointweetretweet = JOIN tweetdetails by tw_id, retweetdetails by tw_id;                                                        
TWEET_RETWEET = FOREACH jointweetretweet GENERATE $0 AS index, $1 AS tw_id, $2 AS mentions, $3 AS rt_id, $4 AS rt_user, $5 AS d
ate, $6 AS context, $7 AS is_media, $8 AS is_retweet,  $9 AS no_likes, $10 AS no_retweets, $11 AS reply, $12 AS text, $13 AS us
er_name;                                                                                                                       
                                                                                                                               
DESCRIBE TWEET_RETWEET;                                                                                                        
STORE TWEET_RETWEET INTO 'root/user/Uni_Assignment/TWEET_RETWEET' USING PigStorage('|');
```
## - ✨Appendix B - Spark✨

```
data = spark.read.format("csv").option("header", True).option("multiLine", True).option("ignoreTrailingWhiteSpace", True).load("hdfs://sandbox-hdp.hortonworks.com:8020/user/maria_dev/TWEET_RETWEET")

from pyspark.sql import Row

csvDF = rdd.map(lambda x: Row(Index = str(x.split("|")[0]),
Context = str(x.split("|")[1]),
Date = str(x.split("|")[2]),
Tweet_ID = str(x.split("|")[3]),
Retweet = str(x.split("|")[5]),
Username = str(x.split("|")[10]))).toDF()

csvDF.show()
```
